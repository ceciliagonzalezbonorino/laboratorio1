{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c132ac7e-9fbd-4235-8776-b3f7b1551d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2 × 6 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td> 613993</td><td>32.8</td><td>1330081</td><td>71.1</td><td>1316916</td><td>70.4</td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>1150099</td><td> 8.8</td><td>8388608</td><td>64.0</td><td>1801065</td><td>13.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 6 of type dbl\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &  613993 & 32.8 & 1330081 & 71.1 & 1316916 & 70.4\\\\\n",
       "\tVcells & 1150099 &  8.8 & 8388608 & 64.0 & 1801065 & 13.8\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 6 of type dbl\n",
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n",
       "|---|---|---|---|---|---|---|\n",
       "| Ncells |  613993 | 32.8 | 1330081 | 71.1 | 1316916 | 70.4 |\n",
       "| Vcells | 1150099 |  8.8 | 8388608 | 64.0 | 1801065 | 13.8 |\n",
       "\n"
      ],
      "text/plain": [
       "       used    (Mb) gc trigger (Mb) max used (Mb)\n",
       "Ncells  613993 32.8 1330081    71.1 1316916  70.4\n",
       "Vcells 1150099  8.8 8388608    64.0 1801065  13.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: data.table\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#limpio la memoria\n",
    "rm( list=ls() )  #remove all objects\n",
    "gc()             #garbage collection\n",
    "\n",
    "require(\"data.table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a55ef1-3d98-42fe-a9ad-3031b4382f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui empieza el programa\n",
    "\n",
    "setwd( \"~/buckets/b1/\" )\n",
    "\n",
    "#cargo el dataset\n",
    "dataset  <- fread(\"exp/FE7250/dataset.transformado2.csv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af994e5c-4a0c-45b6-8e63-093e6c7d4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros del script\n",
    "kexperimento  <- \"TS7310_vm2\"\n",
    "kexp_input  <- \"FE7250_vm2\"\n",
    "\n",
    "#creo la carpeta donde va el experimento\n",
    "dir.create( paste0( \"./exp/\", kexperimento, \"/\"), showWarnings = FALSE )\n",
    "setwd(paste0( \"./exp/\", kexperimento, \"/\"))   #Establezco el Working Directory DEL EXPERIMENTO\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "options(error = function() { \n",
    "  traceback(20); \n",
    "  options(error = NULL); \n",
    "  stop(\"exiting after script error\") \n",
    "})\n",
    "\n",
    "setorder( dataset, foto_mes, numero_de_cliente )\n",
    "\n",
    "kfuture       <- c( 202105 )\n",
    "\n",
    "#grabo los datos del futuro\n",
    "fwrite( dataset[ foto_mes %in% kfuture, ],\n",
    "        file= \"dataset_future_vm2.csv.gz\",\n",
    "        logical01= TRUE,\n",
    "        sep= \",\" )\n",
    "\n",
    "\n",
    "kfinal_train  <- c( 202103, 202102,202101)\n",
    "\n",
    "ktraining     <-  c(202103, 202102,202101)\n",
    "kvalidation   <-  c( 202103, 202102,202101)\n",
    "ktesting      <-  c( 202103, 202102,202101)\n",
    "\n",
    "\n",
    "# FIN Parametros del script\n",
    "\n",
    "#grabo los datos donde voy a entrenar los Final Models\n",
    "fwrite( dataset[ foto_mes %in% kfuture, ],\n",
    "        file= \"dataset_train_final_vm2.csv.gz\",\n",
    "        logical01= TRUE,\n",
    "        sep= \",\" )\n",
    "\n",
    "\n",
    "#grabo los datos donde voy a hacer el training y la optimizacion de hiperparametros\n",
    "dataset[  , fold_train := 0L ]\n",
    "dataset[ foto_mes %in% ktraining, fold_train := 1L ]\n",
    "\n",
    "dataset[  , fold_validate := 0L ]\n",
    "dataset[ foto_mes %in% kvalidation, fold_validate := 1L ]\n",
    "\n",
    "dataset[  , fold_test := 0L ]\n",
    "dataset[ foto_mes %in% ktesting, fold_test := 1L ]\n",
    "\n",
    "fwrite( dataset[ fold_train + fold_validate + fold_test >= 1 , ],\n",
    "        file= \"dataset_training_vm2.csv.gz\",\n",
    "        logical01= TRUE,\n",
    "        sep= \",\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfceed4-95b0-4c67-9494-a738ea79547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: rlist\n",
      "\n",
      "Loading required package: lightgbm\n",
      "\n",
      "Loading required package: DiceKriging\n",
      "\n",
      "Loading required package: mlrMBO\n",
      "\n",
      "Loading required package: mlr\n",
      "\n",
      "Loading required package: ParamHelpers\n",
      "\n",
      "Warning message: 'mlr' is in 'maintenance-only' mode since July 2019.\n",
      "Future development will only happen in 'mlr3'\n",
      "(<https://mlr3.mlr-org.com>). Due to the focus on 'mlr3' there might be\n",
      "uncaught bugs meanwhile in {mlr} - please consider switching.\n",
      "\n",
      "Loading required package: smoof\n",
      "\n",
      "Loading required package: checkmate\n",
      "\n",
      "\n",
      "Attaching package: ‘checkmate’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:DiceKriging’:\n",
      "\n",
      "    checkNames\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“no DISPLAY variable so Tk is not available”\n"
     ]
    }
   ],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "\n",
    "require(\"lightgbm\")\n",
    "\n",
    "#paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7e30a9-cfe6-42ac-aca1-ac02bb6d7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros del script\n",
    "kexperimento  <- \"HT7412_vm2\"\n",
    "kexp_input  <- \"TS7312_vm2\"\n",
    "# FIN Parametros del script\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "options(error = function() { \n",
    "  traceback(20); \n",
    "  options(error = NULL); \n",
    "  stop(\"exiting after script error\") \n",
    "})\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "ksemilla  <- 690621\n",
    "\n",
    "kcrossvalidation_folds  <- 5  #En caso que se haga cross validation, se usa esta cantidad de folds\n",
    "\n",
    "#Hiperparametros FIJOS de  lightgbm\n",
    "param_lgb_basicos  <- list( \n",
    "   boosting= \"gbdt\",          #puede ir  dart  , ni pruebe random_forest\n",
    "   objective= \"binary\",\n",
    "   metric= \"custom\",\n",
    "   first_metric_only= TRUE,\n",
    "   boost_from_average= TRUE,\n",
    "   feature_pre_filter= FALSE,\n",
    "   force_row_wise= TRUE,      #para que los alumnos no se atemoricen con tantos warning\n",
    "   verbosity= -100,\n",
    "   max_depth=  -1,            # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "   min_gain_to_split= 0.0,    #por ahora, lo dejo fijo\n",
    "   lambda_l1= 0.0,            #por ahora, lo dejo fijo\n",
    "   lambda_l2= 0.0,            #por ahora, lo dejo fijo\n",
    "   max_bin= 31,               #por ahora, lo dejo fijo\n",
    "   num_iterations= 9999,      #un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "   bagging_fraction= 1.0,     #por ahora, lo dejo fijo\n",
    "   pos_bagging_fraction= 1.0, #por ahora, lo dejo fijo\n",
    "   neg_bagging_fraction= 1.0, #por ahora, lo dejo fijo\n",
    "\n",
    "   drop_rate=  0.1,           #solo se activa en  dart\n",
    "   max_drop= 50,              #solo se activa en  dart\n",
    "   skip_drop= 0.5,            #solo se activa en  dart\n",
    "\n",
    "   extra_trees= FALSE,\n",
    "\n",
    "   seed=  ksemilla\n",
    "   )\n",
    "\n",
    "\n",
    "#Aqui se cargan los hiperparametros que se optimizan en la Bayesian Optimization\n",
    "hs <- makeParamSet( \n",
    "         makeNumericParam(\"learning_rate\",    lower=    0.005, upper=    0.3),\n",
    "         makeNumericParam(\"feature_fraction\", lower=    0.2  , upper=    1.0),\n",
    "         makeIntegerParam(\"min_data_in_leaf\", lower=    0L   , upper=  8000L),\n",
    "         makeIntegerParam(\"num_leaves\",       lower=   16L   , upper=  2048L)\n",
    "        )\n",
    "\n",
    "\n",
    "#si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "kBO_iteraciones  <- 75  #iteraciones de la Optimizacion Bayesiana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacdcf3e-8aa9-46d7-8f34-587a599b9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#graba a un archivo los componentes de lista\n",
    "#para el primer registro, escribe antes los titulos\n",
    "\n",
    "exp_log  <- function( reg, arch=NA, folder=\"./exp/\", ext=\".txt\", verbose=TRUE )\n",
    "{\n",
    "  archivo  <- arch\n",
    "  if( is.na(arch) )  archivo  <- paste0(  folder, substitute( reg), ext )\n",
    "\n",
    "  if( !file.exists( archivo ) )  #Escribo los titulos\n",
    "  {\n",
    "    linea  <- paste0( \"fecha\\t\", \n",
    "                      paste( list.names(reg), collapse=\"\\t\" ), \"\\n\" )\n",
    "\n",
    "    cat( linea, file=archivo )\n",
    "  }\n",
    "\n",
    "  linea  <- paste0( format(Sys.time(), \"%Y%m%d %H%M%S\"),  \"\\t\",     #la fecha y hora\n",
    "                    gsub( \", \", \"\\t\", toString( reg ) ),  \"\\n\" )\n",
    "\n",
    "  cat( linea, file=archivo, append=TRUE )  #grabo al archivo\n",
    "\n",
    "  if( verbose )  cat( linea )   #imprimo por pantalla\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "vprob_optima  <- c()\n",
    "\n",
    "fganancia_lgbm_meseta  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 , 78000, -2000  ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "\n",
    "  gan  <-  tbl[ , max(gan_acum) ]\n",
    "\n",
    "  pos  <- which.max(  tbl[ , gan_acum ] ) \n",
    "  vprob_optima  <<- c( vprob_optima, tbl[ pos, prob ] )\n",
    "\n",
    "  rm( tbl )\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  param_completo  <- c( param_lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  vprob_optima  <<- c()\n",
    "  set.seed( param_completo$seed )\n",
    "  modelo_train  <- lgb.train( data= dtrain,\n",
    "                              valids= list( valid= dvalidate ),\n",
    "                              eval=   fganancia_lgbm_meseta,\n",
    "                              param=  param_completo,\n",
    "                              verbose= -100 )\n",
    "\n",
    "  prob_corte  <- vprob_optima[ modelo_train$best_iter ]\n",
    "\n",
    "  #aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion  <- predict( modelo_train, \n",
    "                          data.matrix( dataset_test[ , campos_buenos, with=FALSE]) )\n",
    "\n",
    "  tbl  <- dataset_test[ , list(clase_ternaria) ]\n",
    "  tbl[ , prob := prediccion ]\n",
    "  ganancia_test  <- tbl[ prob >= prob_corte, \n",
    "                         sum( ifelse(clase_ternaria==\"BAJA+2\", 78000, -2000 ) )]\n",
    "\n",
    "  cantidad_test_normalizada  <- tbl[ prob >= prob_corte, .N ]\n",
    "\n",
    "  rm( tbl )\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada  <- ganancia_test\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_test_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_test_normalizada\n",
    "    tb_importancia    <- as.data.table( lgb.importance( modelo_train ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_vm2\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "\n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelo_train$best_iter\n",
    "  xx$prob_corte  <- prob_corte\n",
    "  xx$estimulos  <- cantidad_test_normalizada\n",
    "  xx$ganancia  <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log_vm2.txt\" )\n",
    "\n",
    "  return( ganancia_test_normalizada )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#esta es la funcion mas mistica de toda la asignatura\n",
    "# sera explicada en  Laboratorio de Implementacion III\n",
    "\n",
    "vprob_optima  <- c()\n",
    "vpos_optima   <- c()\n",
    "\n",
    "fganancia_lgbm_mesetaCV  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "  vpesos   <- get_field(datos, \"weight\")\n",
    "\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 & vpesos > 1,\n",
    "                                               78000,\n",
    "                                               -2000  ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "\n",
    "  gan  <-  tbl[ , max(gan_acum) ]\n",
    "\n",
    "  pos  <- which.max(  tbl[ , gan_acum ] ) \n",
    "  vpos_optima   <<- c( vpos_optima, pos )\n",
    "  vprob_optima  <<- c( vprob_optima, tbl[ pos, prob ] )\n",
    "\n",
    "  rm( tbl )\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbmCV  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  param_completo  <- c(param_lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  vprob_optima  <<- c()\n",
    "  vpos_optima   <<- c()\n",
    "\n",
    "  set.seed( param_completo$seed )\n",
    "  modelocv  <- lgb.cv( data= dtrain,\n",
    "                       eval=   fganancia_lgbm_mesetaCV,\n",
    "                       param=  param_completo,\n",
    "                       stratified= TRUE,                   #sobre el cross validation\n",
    "                       nfold= kcrossvalidation_folds,\n",
    "                       verbose= -100 )\n",
    "\n",
    "  desde  <- (modelocv$best_iter-1)*kcrossvalidation_folds + 1\n",
    "  hasta  <- desde + kcrossvalidation_folds -1\n",
    "\n",
    "  prob_corte            <-  mean( vprob_optima[ desde:hasta ] )\n",
    "  cantidad_normalizada  <-  mean( vpos_optima[ desde:hasta ] ) * kcrossvalidation_folds\n",
    "\n",
    "  ganancia  <- unlist(modelocv$record_evals$valid$ganancia$eval)[ modelocv$best_iter ]\n",
    "  ganancia_normalizada  <- ganancia * kcrossvalidation_folds\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_normalizada\n",
    "\n",
    "    param_impo <-  copy( param_completo )\n",
    "    param_impo$early_stopping_rounds  <- 0\n",
    "    param_impo$num_iterations  <- modelocv$best_iter\n",
    "\n",
    "    modelo  <- lgb.train( data= dtrain,\n",
    "                       param=  param_impo,\n",
    "                       verbose= -100 )\n",
    "\n",
    "    tb_importancia    <- as.data.table( lgb.importance( modelo ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_vm2\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "    \n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelocv$best_iter\n",
    "  xx$prob_corte  <-  prob_corte\n",
    "  xx$estimulos   <-  cantidad_normalizada\n",
    "  xx$ganancia  <- ganancia_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log_vm2.txt\" )\n",
    "\n",
    "  return( ganancia_normalizada )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6c649f-fc7e-4bcc-95bb-6e37e1f98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui empieza el programa\n",
    "\n",
    "setwd(\"~/buckets/b1/\")\n",
    "\n",
    "#cargo el dataset donde voy a entrenar\n",
    "#esta en la carpeta del exp_input y siempre se llama  dataset_training.csv.gz\n",
    "#dataset_input  <- paste0( \"./exp/\", kexp_input, \"/dataset_training.csv.gz\" )\n",
    "dataset  <- fread( \"exp/TS7310_vm2/dataset_training_vm2.csv.gz\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64afdf31-ea57-491d-805c-3705dfcb2bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2 × 6 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Ncells</th><td>  2236784</td><td> 119.5</td><td>   3676827</td><td>  196.4</td><td>   3676827</td><td>  196.4</td></tr>\n",
       "\t<tr><th scope=row>Vcells</th><td>291511428</td><td>2224.1</td><td>3923902276</td><td>29937.0</td><td>4903326507</td><td>37409.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 6 of type dbl\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n",
       "\\hline\n",
       "\tNcells &   2236784 &  119.5 &    3676827 &   196.4 &    3676827 &   196.4\\\\\n",
       "\tVcells & 291511428 & 2224.1 & 3923902276 & 29937.0 & 4903326507 & 37409.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 6 of type dbl\n",
       "\n",
       "| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n",
       "|---|---|---|---|---|---|---|\n",
       "| Ncells |   2236784 |  119.5 |    3676827 |   196.4 |    3676827 |   196.4 |\n",
       "| Vcells | 291511428 | 2224.1 | 3923902276 | 29937.0 | 4903326507 | 37409.5 |\n",
       "\n"
      ],
      "text/plain": [
       "       used      (Mb)   gc trigger (Mb)    max used   (Mb)   \n",
       "Ncells   2236784  119.5    3676827   196.4    3676827   196.4\n",
       "Vcells 291511428 2224.1 3923902276 29937.0 4903326507 37409.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Verificaciones\n",
    "if( ! (\"fold_train\"    %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_train \\n\")\n",
    "if( ! (\"fold_validate\" %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_validate \\n\")\n",
    "if( ! (\"fold_test\"     %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_test  \\n\")\n",
    "if( dataset[ fold_train==1, .N ] == 0 ) stop(\"Error, en el dataset no hay fold_train==1 \\n\")\n",
    "\n",
    "#creo la carpeta donde va el experimento\n",
    "dir.create( paste0( \"./exp/\", kexperimento, \"/\"), showWarnings = FALSE )\n",
    "setwd(paste0( \"./exp/\", kexperimento, \"/\"))   #Establezco el Working Directory DEL EXPERIMENTO\n",
    "\n",
    "cat( kexp_input,\n",
    "     file= \"TrainingStrategy_vm2.txt\",\n",
    "     append= FALSE )\n",
    "\n",
    "#defino la clase binaria clase01\n",
    "dataset[  , clase01 := ifelse( clase_ternaria==\"CONTINUA\", 0L, 1L ) ]\n",
    "\n",
    "\n",
    "#los campos que se pueden utilizar para la prediccion\n",
    "campos_buenos  <- setdiff( copy(colnames( dataset )), c( \"clase01\", \"clase_ternaria\", \"fold_train\", \"fold_validate\", \"fold_test\" ) )\n",
    "\n",
    "#la particion de train siempre va\n",
    "dtrain  <- lgb.Dataset( data=    data.matrix( dataset[ fold_train==1, campos_buenos, with=FALSE] ),\n",
    "                        label=   dataset[ fold_train==1, clase01 ],\n",
    "                        weight=  dataset[ fold_train==1, ifelse( clase_ternaria == \"BAJA+2\", 1.0000001, 1.0) ],\n",
    "                        free_raw_data= FALSE\n",
    "                      )\n",
    "\n",
    "\n",
    "kvalidate  <- FALSE\n",
    "ktest  <- FALSE\n",
    "kcrossvalidation  <- TRUE\n",
    "\n",
    "#Si hay que hacer validacion\n",
    "if( dataset[ fold_train==0 & fold_test==0 & fold_validate==1, .N ] > 0 )\n",
    "{\n",
    "  kcrossvalidation  <- FALSE\n",
    "  kvalidate  <- TRUE\n",
    "  dvalidate  <- lgb.Dataset( data=  data.matrix( dataset[ fold_validate==1, campos_buenos, with=FALSE] ),\n",
    "                             label= dataset[ fold_validate==1, clase01 ],\n",
    "                             free_raw_data= FALSE  )\n",
    "\n",
    "}\n",
    "\n",
    "#Si hay que hacer testing\n",
    "if( dataset[ fold_train==0 & fold_validate==0 & fold_test==1, .N ] > 0 )\n",
    "{\n",
    "  ktest  <- TRUE\n",
    "  kcrossvalidation  <- FALSE\n",
    "  dataset_test  <- dataset[ fold_test== 1 ]\n",
    "}\n",
    "\n",
    "\n",
    "#Si hay testing, sin validation,  STOP !!\n",
    "if( kvalidate== FALSE & ktest== TRUE ) stop(\"Error, si hay testing, debe haber validation \\n\") \n",
    "\n",
    "\n",
    "rm( dataset )\n",
    "gc()\n",
    "\n",
    "\n",
    "#si ya existe el archivo log, traigo hasta donde procese\n",
    "if( file.exists( \"BO_log_vm2.txt\" ) )\n",
    "{\n",
    "  tabla_log  <- fread( \"BO_log_vm2.txt\" )\n",
    "  GLOBAL_iteracion  <- nrow( tabla_log )\n",
    "  GLOBAL_ganancia   <- tabla_log[ , max(ganancia) ]\n",
    "  rm(tabla_log)\n",
    "} else  {\n",
    "  GLOBAL_iteracion  <- 0\n",
    "  GLOBAL_ganancia   <- -Inf\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d76427-60d7-4fcb-b9ed-f93aed2117c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing y column(s) for design. Not provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aqui comienza la configuracion de mlrMBO\n",
    "\n",
    "#deobo hacer cross validation o  Train/Validate/Test\n",
    "if( kcrossvalidation ) {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbmCV\n",
    "} else {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbm\n",
    "}\n",
    "\n",
    "\n",
    "configureMlr( show.learner.output= FALSE)\n",
    "\n",
    "#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "#por favor, no desesperarse por lo complejo\n",
    "obj.fun  <- makeSingleObjectiveFunction(\n",
    "              fn=       funcion_optimizar, #la funcion que voy a maximizar\n",
    "              minimize= FALSE,   #estoy Maximizando la ganancia\n",
    "              noisy=    TRUE,\n",
    "              par.set=  hs,     #definido al comienzo del programa\n",
    "              has.simple.signature = FALSE   #paso los parametros en una lista\n",
    "             )\n",
    "\n",
    "#archivo donde se graba y cada cuantos segundos\n",
    "ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  \n",
    "                         save.file.path=       \"bayesiana_vm2.RDATA\" )\n",
    "                         \n",
    "ctrl  <- setMBOControlTermination( ctrl, \n",
    "                                   iters= kBO_iteraciones )   #cantidad de iteraciones\n",
    "                                   \n",
    "ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI() )\n",
    "\n",
    "#establezco la funcion que busca el maximo\n",
    "surr.km  <- makeLearner(\"regr.km\",\n",
    "                        predict.type= \"se\",\n",
    "                        covtype= \"matern3_2\",\n",
    "                        control= list(trace= TRUE) )\n",
    "\n",
    "\n",
    "\n",
    "#Aqui inicio la optimizacion bayesiana\n",
    "if( !file.exists( \"bayesiana_vm2.RDATA\" ) ) {\n",
    "\n",
    "  run  <- mbo(obj.fun, learner= surr.km, control= ctrl)\n",
    "\n",
    "} else {\n",
    "  #si ya existe el archivo RDATA, debo continuar desde el punto hasta donde llegue\n",
    "  #  usado para cuando se corta la virtual machine\n",
    "  run  <- mboContinue( \"bayesiana_vm2.RDATA\" )   #retomo en caso que ya exista\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10772265-f910-4323-be16-80c631dab477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
