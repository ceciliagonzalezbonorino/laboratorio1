{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a1891-5793-478e-a6f8-ba7e4d771d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesita para correr en Google Cloud\n",
    "# 128 GB de memoria RAM\n",
    "# 256 GB de espacio en el disco local\n",
    "#   8 vCPU\n",
    "\n",
    "#limpio la memoria\n",
    "rm( list=ls() )  #remove all objects\n",
    "gc()             #garbage collection\n",
    "\n",
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "\n",
    "require(\"lightgbm\")\n",
    "\n",
    "#paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "\n",
    "#Parametros del script\n",
    "kexperimento  <- \"HT7414\"\n",
    "kexp_input  <- \"TS7314\"\n",
    "# FIN Parametros del script\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "options(error = function() { \n",
    "  traceback(20); \n",
    "  options(error = NULL); \n",
    "  stop(\"exiting after script error\") \n",
    "})\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "ksemilla  <- 690621\n",
    "\n",
    "\n",
    "kcrossvalidation_folds  <- 5  #En caso que se haga cross validation, se usa esta cantidad de folds\n",
    "\n",
    "#Hiperparametros FIJOS de  lightgbm\n",
    "param_lgb_basicos  <- list( \n",
    "   boosting= \"gbdt\",          #puede ir  dart  , ni pruebe random_forest\n",
    "   objective= \"binary\",\n",
    "   metric= \"custom\",\n",
    "   first_metric_only= TRUE,\n",
    "   boost_from_average= TRUE,\n",
    "   feature_pre_filter= FALSE,\n",
    "   force_row_wise= TRUE,      #para que los alumnos no se atemoricen con tantos warning\n",
    "   verbosity= -100,\n",
    "   max_depth=  -1,            # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "   min_gain_to_split= 0.0,    #por ahora, lo dejo fijo\n",
    "   lambda_l1= 0.0,            #por ahora, lo dejo fijo\n",
    "   lambda_l2= 0.0,            #por ahora, lo dejo fijo\n",
    "   max_bin= 31,               #por ahora, lo dejo fijo\n",
    "   num_iterations= 9999,      #un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "   bagging_fraction= 1.0,     #por ahora, lo dejo fijo\n",
    "   pos_bagging_fraction= 1.0, #por ahora, lo dejo fijo\n",
    "   neg_bagging_fraction= 1.0, #por ahora, lo dejo fijo\n",
    "\n",
    "   drop_rate=  0.1,           #solo se activa en  dart\n",
    "   max_drop= 50,              #solo se activa en  dart\n",
    "   skip_drop= 0.5,            #solo se activa en  dart\n",
    "\n",
    "   extra_trees= FALSE,\n",
    "\n",
    "   seed=  ksemilla\n",
    "   )\n",
    "\n",
    "\n",
    "#Aqui se cargan los hiperparametros que se optimizan en la Bayesian Optimization\n",
    "hs <- makeParamSet( \n",
    "         makeNumericParam(\"learning_rate\",    lower=    0.005, upper=    0.3),\n",
    "         makeNumericParam(\"feature_fraction\", lower=    0.2  , upper=    1.0),\n",
    "         makeIntegerParam(\"min_data_in_leaf\", lower=    0L   , upper=  8000L),\n",
    "         makeIntegerParam(\"num_leaves\",       lower=   16L   , upper=  2048L)\n",
    "        )\n",
    "\n",
    "\n",
    "#si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "kBO_iteraciones  <- 50  #iteraciones de la Optimizacion Bayesiana\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#graba a un archivo los componentes de lista\n",
    "#para el primer registro, escribe antes los titulos\n",
    "\n",
    "exp_log  <- function( reg, arch=NA, folder=\"./exp/\", ext=\".txt\", verbose=TRUE )\n",
    "{\n",
    "  archivo  <- arch\n",
    "  if( is.na(arch) )  archivo  <- paste0(  folder, substitute( reg), ext )\n",
    "\n",
    "  if( !file.exists( archivo ) )  #Escribo los titulos\n",
    "  {\n",
    "    linea  <- paste0( \"fecha\\t\", \n",
    "                      paste( list.names(reg), collapse=\"\\t\" ), \"\\n\" )\n",
    "\n",
    "    cat( linea, file=archivo )\n",
    "  }\n",
    "\n",
    "  linea  <- paste0( format(Sys.time(), \"%Y%m%d %H%M%S\"),  \"\\t\",     #la fecha y hora\n",
    "                    gsub( \", \", \"\\t\", toString( reg ) ),  \"\\n\" )\n",
    "\n",
    "  cat( linea, file=archivo, append=TRUE )  #grabo al archivo\n",
    "\n",
    "  if( verbose )  cat( linea )   #imprimo por pantalla\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "vprob_optima  <- c()\n",
    "\n",
    "fganancia_lgbm_meseta  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 , 78000, -2000  ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "\n",
    "  gan  <-  tbl[ , max(gan_acum) ]\n",
    "\n",
    "  pos  <- which.max(  tbl[ , gan_acum ] ) \n",
    "  vprob_optima  <<- c( vprob_optima, tbl[ pos, prob ] )\n",
    "\n",
    "  rm( tbl )\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  param_completo  <- c( param_lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  vprob_optima  <<- c()\n",
    "  set.seed( param_completo$seed )\n",
    "  modelo_train  <- lgb.train( data= dtrain,\n",
    "                              valids= list( valid= dvalidate ),\n",
    "                              eval=   fganancia_lgbm_meseta,\n",
    "                              param=  param_completo,\n",
    "                              verbose= -100 )\n",
    "\n",
    "  prob_corte  <- vprob_optima[ modelo_train$best_iter ]\n",
    "\n",
    "  #aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion  <- predict( modelo_train, \n",
    "                          data.matrix( dataset_test[ , campos_buenos, with=FALSE]) )\n",
    "\n",
    "  tbl  <- dataset_test[ , list(clase_ternaria) ]\n",
    "  tbl[ , prob := prediccion ]\n",
    "  ganancia_test  <- tbl[ prob >= prob_corte, \n",
    "                         sum( ifelse(clase_ternaria==\"BAJA+2\", 78000, -2000 ) )]\n",
    "\n",
    "  cantidad_test_normalizada  <- tbl[ prob >= prob_corte, .N ]\n",
    "\n",
    "  rm( tbl )\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada  <- ganancia_test\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_test_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_test_normalizada\n",
    "    tb_importancia    <- as.data.table( lgb.importance( modelo_train ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "\n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelo_train$best_iter\n",
    "  xx$prob_corte  <- prob_corte\n",
    "  xx$estimulos  <- cantidad_test_normalizada\n",
    "  xx$ganancia  <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log.txt\" )\n",
    "\n",
    "  return( ganancia_test_normalizada )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#esta es la funcion mas mistica de toda la asignatura\n",
    "# sera explicada en  Laboratorio de Implementacion III\n",
    "\n",
    "vprob_optima  <- c()\n",
    "vpos_optima   <- c()\n",
    "\n",
    "fganancia_lgbm_mesetaCV  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "  vpesos   <- get_field(datos, \"weight\")\n",
    "\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 & vpesos > 1,\n",
    "                                               78000,\n",
    "                                               -2000  ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "\n",
    "  gan  <-  tbl[ , max(gan_acum) ]\n",
    "\n",
    "  pos  <- which.max(  tbl[ , gan_acum ] ) \n",
    "  vpos_optima   <<- c( vpos_optima, pos )\n",
    "  vprob_optima  <<- c( vprob_optima, tbl[ pos, prob ] )\n",
    "\n",
    "  rm( tbl )\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbmCV  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  param_completo  <- c(param_lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  vprob_optima  <<- c()\n",
    "  vpos_optima   <<- c()\n",
    "\n",
    "  set.seed( param_completo$seed )\n",
    "  modelocv  <- lgb.cv( data= dtrain,\n",
    "                       eval=   fganancia_lgbm_mesetaCV,\n",
    "                       param=  param_completo,\n",
    "                       stratified= TRUE,                   #sobre el cross validation\n",
    "                       nfold= kcrossvalidation_folds,\n",
    "                       verbose= -100 )\n",
    "\n",
    "  desde  <- (modelocv$best_iter-1)*kcrossvalidation_folds + 1\n",
    "  hasta  <- desde + kcrossvalidation_folds -1\n",
    "\n",
    "  prob_corte            <-  mean( vprob_optima[ desde:hasta ] )\n",
    "  cantidad_normalizada  <-  mean( vpos_optima[ desde:hasta ] ) * kcrossvalidation_folds\n",
    "\n",
    "  ganancia  <- unlist(modelocv$record_evals$valid$ganancia$eval)[ modelocv$best_iter ]\n",
    "  ganancia_normalizada  <- ganancia * kcrossvalidation_folds\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_normalizada\n",
    "\n",
    "    param_impo <-  copy( param_completo )\n",
    "    param_impo$early_stopping_rounds  <- 0\n",
    "    param_impo$num_iterations  <- modelocv$best_iter\n",
    "\n",
    "    modelo  <- lgb.train( data= dtrain,\n",
    "                       param=  param_impo,\n",
    "                       verbose= -100 )\n",
    "\n",
    "    tb_importancia    <- as.data.table( lgb.importance( modelo ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "    \n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelocv$best_iter\n",
    "  xx$prob_corte  <-  prob_corte\n",
    "  xx$estimulos   <-  cantidad_normalizada\n",
    "  xx$ganancia  <- ganancia_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log.txt\" )\n",
    "\n",
    "  return( ganancia_normalizada )\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------\n",
    "#Aqui empieza el programa\n",
    "\n",
    "setwd(\"~/buckets/b1/\")\n",
    "\n",
    "#cargo el dataset donde voy a entrenar\n",
    "#esta en la carpeta del exp_input y siempre se llama  dataset_training.csv.gz\n",
    "dataset_input  <- paste0( \"./exp/\", kexp_input, \"/dataset_training.csv.gz\" )\n",
    "dataset  <- fread( dataset_input )\n",
    "\n",
    "#Verificaciones\n",
    "if( ! (\"fold_train\"    %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_train \\n\")\n",
    "if( ! (\"fold_validate\" %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_validate \\n\")\n",
    "if( ! (\"fold_test\"     %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_test  \\n\")\n",
    "if( dataset[ fold_train==1, .N ] == 0 ) stop(\"Error, en el dataset no hay fold_train==1 \\n\")\n",
    "\n",
    "#creo la carpeta donde va el experimento\n",
    "dir.create( paste0( \"./exp/\", kexperimento, \"/\"), showWarnings = FALSE )\n",
    "setwd(paste0( \"./exp/\", kexperimento, \"/\"))   #Establezco el Working Directory DEL EXPERIMENTO\n",
    "\n",
    "cat( kexp_input,\n",
    "     file= \"TrainingStrategy.txt\",\n",
    "     append= FALSE )\n",
    "\n",
    "#defino la clase binaria clase01\n",
    "dataset[  , clase01 := ifelse( clase_ternaria==\"CONTINUA\", 0L, 1L ) ]\n",
    "\n",
    "\n",
    "#los campos que se pueden utilizar para la prediccion\n",
    "campos_buenos  <- setdiff( copy(colnames( dataset )), c( \"clase01\", \"clase_ternaria\", \"fold_train\", \"fold_validate\", \"fold_test\" ) )\n",
    "\n",
    "#la particion de train siempre va\n",
    "dtrain  <- lgb.Dataset( data=    data.matrix( dataset[ fold_train==1, campos_buenos, with=FALSE] ),\n",
    "                        label=   dataset[ fold_train==1, clase01 ],\n",
    "                        weight=  dataset[ fold_train==1, ifelse( clase_ternaria == \"BAJA+2\", 1.0000001, 1.0) ],\n",
    "                        free_raw_data= FALSE\n",
    "                      )\n",
    "\n",
    "\n",
    "kvalidate  <- FALSE\n",
    "ktest  <- FALSE\n",
    "kcrossvalidation  <- TRUE\n",
    "\n",
    "#Si hay que hacer validacion\n",
    "if( dataset[ fold_train==0 & fold_test==0 & fold_validate==1, .N ] > 0 )\n",
    "{\n",
    "  kcrossvalidation  <- FALSE\n",
    "  kvalidate  <- TRUE\n",
    "  dvalidate  <- lgb.Dataset( data=  data.matrix( dataset[ fold_validate==1, campos_buenos, with=FALSE] ),\n",
    "                             label= dataset[ fold_validate==1, clase01 ],\n",
    "                             free_raw_data= FALSE  )\n",
    "\n",
    "}\n",
    "\n",
    "#Si hay que hacer testing\n",
    "if( dataset[ fold_train==0 & fold_validate==0 & fold_test==1, .N ] > 0 )\n",
    "{\n",
    "  ktest  <- TRUE\n",
    "  kcrossvalidation  <- FALSE\n",
    "  dataset_test  <- dataset[ fold_test== 1 ]\n",
    "}\n",
    "\n",
    "\n",
    "#Si hay testing, sin validation,  STOP !!\n",
    "if( kvalidate== FALSE & ktest== TRUE ) stop(\"Error, si hay testing, debe haber validation \\n\") \n",
    "\n",
    "\n",
    "rm( dataset )\n",
    "gc()\n",
    "\n",
    "\n",
    "#si ya existe el archivo log, traigo hasta donde procese\n",
    "if( file.exists( \"BO_log.txt\" ) )\n",
    "{\n",
    "  tabla_log  <- fread( \"BO_log.txt\" )\n",
    "  GLOBAL_iteracion  <- nrow( tabla_log )\n",
    "  GLOBAL_ganancia   <- tabla_log[ , max(ganancia) ]\n",
    "  rm(tabla_log)\n",
    "} else  {\n",
    "  GLOBAL_iteracion  <- 0\n",
    "  GLOBAL_ganancia   <- -Inf\n",
    "}\n",
    "\n",
    "\n",
    "#Aqui comienza la configuracion de mlrMBO\n",
    "\n",
    "#deobo hacer cross validation o  Train/Validate/Test\n",
    "if( kcrossvalidation ) {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbmCV\n",
    "} else {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbm\n",
    "}\n",
    "\n",
    "\n",
    "configureMlr( show.learner.output= FALSE)\n",
    "\n",
    "#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "#por favor, no desesperarse por lo complejo\n",
    "obj.fun  <- makeSingleObjectiveFunction(\n",
    "              fn=       funcion_optimizar, #la funcion que voy a maximizar\n",
    "              minimize= FALSE,   #estoy Maximizando la ganancia\n",
    "              noisy=    TRUE,\n",
    "              par.set=  hs,     #definido al comienzo del programa\n",
    "              has.simple.signature = FALSE   #paso los parametros en una lista\n",
    "             )\n",
    "\n",
    "#archivo donde se graba y cada cuantos segundos\n",
    "ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  \n",
    "                         save.file.path=       \"bayesiana.RDATA\" )\n",
    "                         \n",
    "ctrl  <- setMBOControlTermination( ctrl, \n",
    "                                   iters= kBO_iteraciones )   #cantidad de iteraciones\n",
    "                                   \n",
    "ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI() )\n",
    "\n",
    "#establezco la funcion que busca el maximo\n",
    "surr.km  <- makeLearner(\"regr.km\",\n",
    "                        predict.type= \"se\",\n",
    "                        covtype= \"matern3_2\",\n",
    "                        control= list(trace= TRUE) )\n",
    "\n",
    "\n",
    "\n",
    "#Aqui inicio la optimizacion bayesiana\n",
    "if( !file.exists( \"bayesiana.RDATA\" ) ) {\n",
    "\n",
    "  run  <- mbo(obj.fun, learner= surr.km, control= ctrl)\n",
    "\n",
    "} else {\n",
    "  #si ya existe el archivo RDATA, debo continuar desde el punto hasta donde llegue\n",
    "  #  usado para cuando se corta la virtual machine\n",
    "  run  <- mboContinue( \"bayesiana.RDATA\" )   #retomo en caso que ya exista\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
